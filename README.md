# Obfuscated-Malware-detection
 
 
                                                           Abstract 
Detecting and blocking malware has become increasingly challenging for antivirus programs and other security measures due to the constantly changing sophisticated obfuscation techniques. This further leads to increased risks for users. Obfuscation is used to hide malicious code within legitimate processes, making it difficult to detect. 
In this project, we propose a machine learning-based approach that uses system call sequences as features to detect different types of malware, such as Ransomware, Trojan, Spyware, and benign (or genuine) processes. We evaluated our model on the dataset of CIS-MalMem2022 and achieved an overall accuracy of 87.09%. The results demonstrate the effectiveness of our approach in detecting different types of malware and benign processes. 
Keywords – Malware, Obfuscation, CTGAN, SMOTE, Hyperparameter Tuning 

                                                          MOTIVATION: - 
The motivation for this project stems from the fact that obfuscated malware has become increasingly prevalent. It is a growing concern in the field of cybersecurity. Attackers continuously find new ways to obfuscate their malicious code to evade detection and achieve their nefarious goals. As a result, this field requires greater attention to develop new techniques and strategies to detect and mitigate obfuscated malware. 
PROBLEM STATEMENT: - 
The problem statement identified is to detect the type of malware and classify them as either “trojan,” “spyware,” or “ransomware,” or even “benign,” based on their various attribute values. 
                                                          OBJECTIVES: - 
Our objective is to identify and classify obfuscated malware effectively. 
                                                           METHODOLOGY: - 
To achieve our target, we have used supervised and unsupervised learning models to train and test on our dataset at different resource levels. We have also used hyperparameter tuning on the different attributes to identify and neglect the outliers in the dataset. 
Implementing all these techniques, we have identified Random Forest as the most suitable model for identifying and classifying the type of malware. 
  
 
DATA ANALYTICS MODEL: - 
  
For the model, we have used two different types of parameters. First, with resource level 0, which uses a basic hyperparameter, for resource level 1, we have improved the hyperparameters. 
ALGORITHM DESCRIPTION: - 
We had initially applied 10 machine learning algorithms, which are mentioned below. To further improve upon the learning of the applied models and the obtained accuracy, we used CTGAN to generate more data and SMOTE to treat data imbalance. 
Finally, we have applied a combination of both CTGAN and SMOTE on our dataset and applied those 10 models again to obtain an accuracy of 94.27%.  
a)	Pre-processing: The drop() function from the panda’s library first removes the 'Class' and 'Category' columns from the input data. The 'Class' column is unnecessary for modeling purposes, while the 'Category' column works as our target variable.  
To predict the category of a sample, we used the replace() function from the panda’s library to replace the categorical labels with numerical labels. 'Benign' was replaced with 0, 'Ransomware' with 1, 'Spyware' with 2, and 'Trojan' with 3. Thus, the categorical labels in the 'Category' column are transformed into numerical labels for the following machine-learning models. 
b)	Splitting the Dataset: To test whether the predicted classes are correct or not, we had to try the trained model. So, we split the existing dataset into train and test based on 70-30% criteria. 
c)	Resources Level: Resources level and number features are the two arguments that the model functions we built takes in as formal parameters, providing the hyperparameters for the models. The hyper param dictionary, which is initialized to an empty dictionary, contains the hyperparameter definitions. 
If resources_level is 0, the function sets the hyperparameters for low resource models. These models are made to be computationally less intensive and use fewer resources. 
While, if resources_level is 1, the function sets the hyperparameters for high resource models, which makes them more computationally intensive and utilizes more resources. 
d)	Logistic Regression: Logistic regression is a statistical method used to analyze and model the relationship between a binary dependent variable and one or more independent variables. The classifier uses the Scikit-learn library for machine learning. It is used to build a model for a classification task. The solver argument is set to 'sag,’ which stands for Stochastic Average Gradient descent. Large-scale logistic regression issues are solved with this solution since it is computationally effective and capable of handling numerous data. The max_iter argument is set to hyper-param["lr_max_iter"], which is the maximum number of iterations for the solver to converge. This value is defined in the hyper param dictionary, which is passed as an argument to the built function. 
The initialized classifier is kept in the dictionary called built models under the “Logistic Regression” key and saved. 
The accuracy obtained at resource level 0 is 52.36%, and at resource level 1 is 52.05%.  
Merit: Logistic regression is simple to implement and interpret. Demerit: Logistic regression assumes a linear relationship between predictors and the log odds of the outcome. 
e)	Support Vector Machine: Support Vector Machine (SVM) is a supervised machine learning algorithm used for classification and regression analysis. This classifier is used to build a model for a classification task. When the kernel option is set to "linear," the SVM will operate using a linear kernel. For data that can be separated linearly, this kernel fits well. The random_state argument is set to cur_random_state to produce a random number of seeds used to ensure the consistency of outcomes. The max_iter argument is set to hyperparam["svc_max_iter"], which is the maximum number of iterations for the solver to converge. 
The built_models dictionary is used to store the initialized classifier. The classifier is stored under the “Support Vector Machine” key in this case. 
The accuracy obtained at resource level 0 is 65.82%, and at resource level 1 is 48.66%. 
Merit: SVM can handle high dimensional data and effectively deals with non-linearly separable data. 
Demerit: SVM can be sensitive to the choice of kernel function and parameters. 
f)	Keras Neural Network:  Keras is an open-source neural network library written in Python designed to enable fast experimentation with deep neural networks. This model is used to build a neural network for the classification task. 
The model consists of four layers. The first layer is a Flatten layer, which is used to flatten the input data into a one-dimensional array. The second layer is a Dense layer with 67 units and a ReLU activation function. The third layer is a Dropout layer with a rate of 0.2, which randomly drops out 20% of the units in the previous layer during training. The fourth layer is a Dense layer with 15 units and a softmax activation function, which outputs a probability distribution over the 15 classes. 
g)	The stochastic gradient descent algorithm known as the Adam optimizer is used to create the model. For multiclass classification problems with integer labels, the loss function is set to sparse_categorical_crossentropy. 
The built_models dictionary is used to store the initialized model. In this case, the model is stored under the key "Keras Neural Network.” The accuracy obtained at resource level 0 is 67.69%, and at resource level 1 is 68.11%. 
Merit: Keras allows for quick and easy prototyping of deep learning models with minimal coding. 
Demerit: Keras can be limited in flexibility and may not be suitable for more complex or customized neural network architectures. 
h)	Naive Bayes: Naive Bayes is a probabilistic algorithm used in machine learning for classification and prediction tasks based on Bayes' theorem. In Naive Bayes, we use the GaussianNB function for our project. This classifier is used to build a model for a classification task. The Gaussian Naive Bayes classifier assumes that the features are normally distributed and are conditionally independent given the class label.  
The nb_classifier object is used to store the initialized classifier. In this case, the classifier is stored under the “Naïve Bayes” key.” 
The accuracy obtained at resource level 0 is 68.57%, and at resource level 1 is 68.50%. 
Merit: Naive Bayes is computationally efficient and can work well with small datasets. 
Demerit: Naive Bayes assumes that all features are independent of each other, which can lead to decreased accuracy in some cases. 
i)	K Nearest Neighbors: It is a non-parametric, supervised learning classifier that uses proximity to make classifications or predictions about the grouping of an individual data point. By identifying the k samples in the training set that are the most similar to the sample under consideration, the KNN classifier predicts the class of the sample by utilizing the predicted class shared by most of these samples. 
The initialized classifier is stored in the KNN classifier object. The number of neighbours to consider when making predictions is indicated by the value given in the hyper-param dictionary in the n neighbors hyperparameter. 
In this case, the classifier is stored under the key "K Nearest Neighbors.” 
The accuracy obtained at resource level 0 is 80.91%, and at resource level 1 is 79.88%. 
Merit: KNN is simple to implement and can work well with nonlinear data and complex decision boundaries. 
Demerit: KNN can be computationally expensive for large datasets and may need to perform better with high-dimensional data. 
j)	Decision Trees: Decision tree is a supervised machine learning algorithm used for classification and regression analysis that partitions the data into a hierarchical tree-like structure based on the features of the data. 
 The initialized classifier is kept in the decision_tree_classifier object. The hyperparameter max depth is set to the maximum depth of the tree, which is the number listed in the hyper-param dictionary. The classifier, in this instance, is saved with the key "Decision Trees.” The accuracy obtained at resource level 0 is 81.70%, and at resource level 1 is 84.33%. 
Merit: Decision trees are easy to understand and interpret and can handle both categorical and numerical data. 
Demerit: Decision trees can be prone to overfitting and may need to perform better with complex data structures. 
k)	Random Forest: Random Forest is an ensemble learning technique in machine learning that builds multiple decision trees and combines their output to improve the accuracy and robustness of the model. The initialized classifier is kept in an object called 
random_forest_classifier. The hyperparameter max_depth is set to the maximum depth of the trees, which is the value listed in the hyperparam dictionary. 
The classifier is kept in this instance under the key "Random Forest.”  The accuracy obtained at resource level 0 is 82.21%, and at resource level 1 is 87.09%. 
Merit: Random Forest can handle high-dimensional data with many features and can handle missing values. 
Demerit: Random forests can be slow to train on large datasets and may need to be more easily interpretable than individual decision trees. 
l) Light Gradient Boosting Machine: Light Gradient Boosting Machine (LightGBM) is a gradient boosting framework that uses tree-based learning algorithms and is designed to be efficient in training and prediction speed, memory usage, and scalability. 
The algorithm is an ensemble learning method that utilizes trees to optimize a differentiable loss function by gradually adding weak learners to the model. This is achieved through a process of iterative improvement, where the model is trained to minimize the loss function by analyzing and improving the performance of its individual components. 
The initialized classifier is kept in an object called lgbm_classifier. The n_estimators hyperparameter in LightGBM specifies the number of boosting iterations (i.e., the number of trees) to perform during training. Increasing the number of trees will improve the model's performance but also increase the risk of overfitting and slow down the training process. 
The accuracy obtained at resource level 0 is 86.14%, and at resource level 1 is 86.77%. 
Merit: LightGBM is highly efficient and can handle large datasets with high dimensionality, making it well-suited for big data applications. Demerit: LightGBM can be sensitive to overfitting and may require careful tuning of hyperparameters to achieve optimal performance. 
m) Catboost: CatBoost is an open-source gradient boosting framework that uses decision trees and is designed to work well with categorical features and handle missing values. 
What sets CatBoost apart is its optimized and efficient handling of categorical features, which are variables with discrete values like text labels or identifiers, during the training process. 
The initialized classifier is kept in an object called catboost_classifier. It has hyperparameter n_estimator, which refers to the count of trees in the ensemble or the number of boosting iterations to be executed during training. It can be considered equivalent to the "number of trees" or the "number of boosting rounds" in other gradient boosting algorithms. 
The accuracy obtained at resource level 0 is 86.14%, and at resource level 1 is 86.77%. 
Merit: CatBoost can handle categorical features and missing values without requiring preprocessing and can achieve high accuracy with small and unbalanced datasets. 
Demerit: CatBoost can be computationally expensive and may require careful tuning of hyperparameters to avoid overfitting. 
n)	Hist Extreme Gradient Boosting:  Hist Gradient Boosting is a variant of gradient boosting that uses histograms to speed up the training process and reduce memory usage while maintaining high accuracy. The max_iter hyperparameter in Scikit-learn's implementation of HistGradientBoosting specifies the maximum number of boosting iterations (i.e., the maximum number of trees) to build during training. Increasing max_iter can improve performance but may lead to overfitting and longer training times. The optimal value of max_iter should be chosen to balance the trade-off between model performance and training time, which can be determined using techniques such as cross-validation. Increasing max_iter improves performance to a certain point beyond which the model overfits. 
The accuracy obtained at resource level 0 is 86.32%, and at resource level 1 is 86.41%. 
Merit: HistGradientBoosting can be faster and more memory-efficient than traditional gradient boosting while maintaining high accuracy, making it suitable for large datasets. 
Demerit: HistGradientBoosting may perform poorly with datasets with low instances or features. 
o)	CTGAN: Known as Conditional Tabular Generative Adversarial Network) is a type of generative adversarial network (GAN) explicitly designed for tabular data. It is a deep learning model that learns the underlying distribution of a dataset and generates synthetic data that is statistically similar to the original data. 
It is useful for data augmentation, where additional synthetic data can be generated to increase the size of a dataset, as well as for preserving privacy by generating synthetic data that does not reveal sensitive information. 
For our dataset, we increase the data by 2 lahks using CTGAN. The no. of data for each class after applying CTGAN is: 
1.	Benign – 127019  
2.	Spyware – 44482  
3.	Ransomeware – 43990  
4.	Trojan – 43105 
The highest accuracy obtained after applying CTGAN on the actual dataset for Resource level 0 is 91.43% for hist extreme gradient boosting, while for Resource level 1, it is 92.17% for hist extreme gradient boosting. 
p)	SMOTE: The Synthetic Minority Over-Sampling Technique creates synthetic examples of the minority class by interpolating between existing samples in the minority class. It selects one instance from the minority class and finds its k nearest neighbors in feature space. It then creates new synthetic examples by interpolating between the selected criterion and its neighbors. 
The attribute amounts are – 1. Benign – 29298 
2.	Spyware – 10020 
3.	Ransomeware – 9791 
4.	Trojan – 9487 
The highest accuracy obtained after applying SMOTE on the actual dataset for Resource level 0 is 89.26% for Keras Neural Network, while for Resource level 1, it is 92.07% for Random Forest. 
Finally, 
To obtain better accuracy, SMOTE was applied to the dataset generated by 
CTGAN. It converted all the values in the “category” column to a total of 127019 records each, as 127019 is the highest no. of data that was obtained for the value Benign. 
The Accuracy obtained after applying it is 94.27% for Keras Neural Network at Resource Level 0, while the accuracy obtained at Resource Level 1 is 94.24% for Random Forest. 
Results & Discussion 
Our project aims to detect and identify the type of malware encountered. Thus, per our goal, we have used ten supervised and unsupervised models to classify and identify the various attribute values available under the feature labeled as “category.” 
The models used are given below, with their training speed and accuracy values also mentioned. 
 	 

The maximum accuracy obtained for prediction using resource level = 0 is 86.32% from the Histogram-Based Gradient Boosting model. While at resource level = 1, it is 87.09% from the Random Forest model. We have further tried to improve upon our model by using CTGAN and SMOTE on our dataset to increase the amount of data and treat data imbalance, respectively. 

  
